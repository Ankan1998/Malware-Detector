
import pandas as pd #used for DATAFrames and DataFrames can hold different types data of multidimensional arrays. 
import numpy as np#Numpy provides robust data structures for efficient computation of multi-dimensional arrays & matrices.
import pickle 
#from sklearn.model_selection import cross_val_score
import sklearn.ensemble as ske 
import joblib
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from xgboost import XGBClassifier
#from sklearn.metrics import confusion_matrix

orig_ds = pd.read_csv('kaggle-data.csv') #generate df as data
orig_ds = orig_ds.drop(['ID', 'md5','Unnamed: 57','Machine'], axis=1) #now droping some coloumns as axis 1(mean coloumn) and will show the values in the rows
#y = data['legitimate'].values #values of legitimate data
#Checking for null && replacing
blist=[]
for items in orig_ds.isnull().sum().iteritems():
    if items[1]>0:
        blist.append(items[0])
for j in blist:
    count=0
    for i in orig_ds[j].value_counts().iteritems():
        count=count+1
        if count==1:
            orig_ds[j].fillna(value=i[1],inplace=True)
#Storing dependent variable
orig_ds_legi=orig_ds[['legitimate']]

#forming final set
Xtrain=orig_ds.iloc[:,:-1]
ytrain=orig_ds_legi.iloc[:,:]

#Feature selection
sel_model_train=SelectFromModel(Lasso(alpha=0.1,random_state=0,tol=0.007,max_iter=1000))
sel_model_train.fit(Xtrain,ytrain)



#Fitting selected model to data
sel_feat=Xtrain.columns[(sel_model_train.get_support())]
Xtrain=Xtrain[sel_feat]

#Final step dataframe to array
Xtrain=Xtrain.iloc[:,:].values
ytrain=ytrain.iloc[:,:].values

#Fitting XGBoost classifier
classifier=XGBClassifier()
AlgoClassifier=classifier.fit(Xtrain,ytrain)


#Persist an arbitrary Python object into one file.
joblib.dump(AlgoClassifier, 'classifier/classifier.pkl')
open('classifier/features.pkl', 'wb').write(pickle.dumps(sel_feat)) 
#joblib works especially well with NumPy arrays which are used by sklearn so depending on the classifier type you use you might have performance and size benefits using joblib.Otherwise pickle does work correctly so saving a trained classifier and loading it again will produce the same results no matter which of the serialization libraries you use
print('Saved')




# Feature selection using Trees Classifier
"""fsel = ske.ExtraTreesClassifier().fit(X, y)
model = SelectFromModel(fsel, prefit=True)
X_new = model.transform(X)#now features are only 9 :)
nb_features = X_new.shape[1]#will save value 13 as shape is (138047, 13) :}"""#####################


